{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the required libraries\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport re\nimport string\nimport operator\nimport collections\nimport itertools\nimport json\n\nfrom flashtext import KeywordProcessor\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split,cross_validate,ShuffleSplit\nfrom sklearn.metrics import accuracy_score, classification_report,confusion_matrix\n\nimport nltk\nen_stop = set(nltk.corpus.stopwords.words('english'))\nfrom nltk.tokenize import word_tokenize\n\nimport re\nimport spacy \nnlp=spacy.load('en_core_web_sm')\nfrom spacy.tokenizer import _get_regex_pattern\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom wordcloud import WordCloud, STOPWORDS\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport fasttext\n\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import precision_score, recall_score, f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Getting the file path\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/tweet-sentiment-extraction/train.csv\")\ndf_test = pd.read_csv(\"../input/tweet-sentiment-extraction/test.csv\")\ndf_sample_submission = pd.read_csv(\"../input/tweet-sentiment-extraction/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **EDA and Data Cleaning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"For training data:\")\nprint(df_train[\"sentiment\"].shape)\nprint(df_train[\"sentiment\"].value_counts())\n# print(df_train.describe)\nprint()\nprint(\"For testing data:\")\nprint(df_test[\"sentiment\"].shape)\nprint(df_test[\"sentiment\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Missing value imputation**"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndf_train[\"text\"]= df_train[\"text\"].str.strip().replace(np.nan, \"\").str.lower()\ndf_train[\"selected_text\"]= df_train[\"selected_text\"].str.strip().replace(np.nan, \"\").str.lower()\ndf_test[\"text\"]= df_test[\"text\"].str.strip().replace(np.nan, \"\").str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def jaccard(str1, str2): \n    try:\n        a = set(str1.split()) \n        b = set(str2.split())\n        c = a.intersection(b)\n        return float(len(c)) / (len(a) + len(b) - len(c))\n    except ZeroDivisionError:\n        return 0\n\ndf_train[\"jaccard_scr\"] = df_train.progress_apply(lambda x: jaccard(x[\"text\"], x[\"selected_text\"]), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train[\"jaccard_scr\"].mean())\nprint(df_train[df_train[\"sentiment\"]==\"neutral\"][\"jaccard_scr\"].mean())\nprint(df_train[df_train[\"sentiment\"]==\"positive\"][\"jaccard_scr\"].mean())\nprint(df_train[df_train[\"sentiment\"]==\"negative\"][\"jaccard_scr\"].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ## creating custom spacy tokeniser\n\n# text_temp = \"I like #fresh *hashtags *** and *hashtags*   hashtags*. Sons of ****, why couldn`t they put them on t \"\n# doc = nlp(text_temp)\n# print([e for e in doc])\n\n# re_token_match = _get_regex_pattern(nlp.Defaults.token_match)\n# re_token_match = f\"({re_token_match}|#\\w+|\\w+\\#|\\#\\w+\\#|\\*\\w+|\\w+\\*|\\*\\w+\\*|\\*+)\"\n# nlp.tokenizer.token_match = re.compile(re_token_match).match\n\n# doc = nlp(text_temp)\n# print([e for e in doc])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Text cleaning\ntext_clean_replace_list = [\n    # Special characters\n    ['\\x89Û_', ''],\n    ['\\x89ÛÒ', ''],\n    ['\\x89ÛÓ', ''],\n    ['\\x89ÛÏWhen', 'When'],\n    ['\\x89ÛÏ', ''],\n    ['China\\x89Ûªs', \"China's\"],\n    ['let\\x89Ûªs', \"let's\"],\n    ['\\x89Û÷', ''],\n    ['\\x89Ûª', ''],\n    ['\\x89Û\\x9d', ''],\n    ['å_', ''],\n    ['\\x89Û¢', ''],\n    ['\\x89Û¢åÊ', ''],\n    ['fromåÊwounds', 'from wounds'],\n    ['åÊ', ''],\n    ['åÈ', ''],\n    ['JapÌ_n', 'Japan'],\n    ['Ì©', 'e'],\n    ['å¨', ''],\n    ['SuruÌ¤', 'Suruc'],\n    ['åÇ', ''],\n    ['å£3million', '3 million'],\n    ['åÀ', ''],\n\n    # Contractions\n    \n    [\"i`m\", 'i am'],\n    [\"he's\", 'he is'],\n    [\"there's\", 'there is'],\n    [\"We're\", 'We are'],\n    [\"That's\", 'That is'],\n    [\"won't\", 'will not'],\n    [\"they're\", 'they are'],\n    [\"Can't\", 'Cannot'],\n    [\"wasn't\", 'was not'],\n    ['don\\x89Ûªt', 'do not'],\n    [\"aren't\", 'are not'],\n    [\"isn't\", 'is not'],\n    [\"What's\", 'What is'],\n    [\"haven't\", 'have not'],\n    [\"hasn't\", 'has not'],\n    [\"There's\", 'There is'],\n    [\"He's\", 'He is'],\n    [\"It's\", 'It is'],\n    [\"You're\", 'You are'],\n    [\"I'M\", 'I am'],\n    [\"shouldn't\", 'should not'],\n    [\"wouldn't\", 'would not'],\n    [\"i'm\", 'I am'],\n    ['I\\x89Ûªm', 'I am'],\n    [\"I'm\", 'I am'],\n    [\"Isn't\", 'is not'],\n    [\"Here's\", 'Here is'],\n    [\"you've\", 'you have'],\n    ['you\\x89Ûªve', 'you have'],\n    [\"we're\", 'we are'],\n    [\"what's\", 'what is'],\n    [\"couldn't\", 'could not'],\n    [\"we've\", 'we have'],\n    ['it\\x89Ûªs', 'it is'],\n    ['doesn\\x89Ûªt', 'does not'],\n    ['It\\x89Ûªs', 'It is'],\n    ['Here\\x89Ûªs', 'Here is'],\n    [\"who's\", 'who is'],\n    ['I\\x89Ûªve', 'I have'],\n    [\"y'all\", 'you all'],\n    ['can\\x89Ûªt', 'cannot'],\n    [\"would've\", 'would have'],\n    [\"it'll\", 'it will'],\n    [\"we'll\", 'we will'],\n    ['wouldn\\x89Ûªt', 'would not'],\n    [\"We've\", 'We have'],\n    [\"he'll\", 'he will'],\n    [\"Y'all\", 'You all'],\n    [\"Weren't\", 'Were not'],\n    [\"Didn't\", 'Did not'],\n    [\"they'll\", 'they will'],\n    [\"they'd\", 'they would'],\n    [\"DON'T\", 'DO NOT'],\n    ['That\\x89Ûªs', 'That is'],\n    [\"they've\", 'they have'],\n    [\"i'd\", 'I would'],\n    [\"should've\", 'should have'],\n    ['You\\x89Ûªre', 'You are'],\n    [\"where's\", 'where is'],\n    ['Don\\x89Ûªt', 'Do not'],\n    [\"we'd\", 'we would'],\n    [\"i'll\", 'I will'],\n    [\"weren't\", 'were not'],\n    [\"They're\", 'They are'],\n    ['Can\\x89Ûªt', 'Cannot'],\n    ['you\\x89Ûªll', 'you will'],\n    ['I\\x89Ûªd', 'I would'],\n    [\"let's\", 'let us'],\n    [\"it's\", 'it is'],\n    [\"can't\", 'cannot'],\n    [\"don't\", 'do not'],\n    [\"you're\", 'you are'],\n    [\"i've\", 'I have'],\n    [\"that's\", 'that is'],\n    [\"i'll\", 'I will'],\n    [\"doesn't\", 'does not'],\n    [\"i'd\", 'I would'],\n    [\"didn't\", 'did not'],\n    [\"ain't\", 'am not'],\n    [\"you'll\", 'you will'],\n    [\"I've\", 'I have'],\n    [\"Don't\", 'do not'],\n    [\"I'll\", 'I will'],\n    [\"I'd\", 'I would'],\n    [\"Let's\", 'Let us'],\n    [\"you'd\", 'You would'],\n    [\"It's\", 'It is'],\n    [\"Ain't\", 'am not'],\n    [\"Haven't\", 'Have not'],\n    [\"Could've\", 'Could have'],\n    ['youve', 'you have'],\n    ['donå«t', 'do not'],\n\n    # Character entity references\n    ['&gt;', '>'],\n    ['&lt;', '<'],\n    ['&amp;', '&'],\n\n    # Typos', 'slang and informal abbreviations\n    ['w/e', 'whatever'],\n    ['w/', 'with'],\n    ['USAgov', 'USA government'],\n    ['recentlu', 'recently'],\n    ['Ph0tos', 'Photos'],\n    ['amirite', 'am I right'],\n    ['exp0sed', 'exposed'],\n    ['<3', 'love'],\n    ['lmao', 'laughing my ass off'],\n    ['thankU', 'thank you'],\n\n    # Acronyms\n    ['MH370', 'Malaysia Airlines Flight 370'],\n    ['mÌ¼sica', 'music'],\n    ['okwx', 'Oklahoma City Weather'],\n    ['arwx', 'Arkansas Weather'],\n    ['gawx', 'Georgia Weather'],\n    ['scwx', 'South Carolina Weather'],\n    ['cawx', 'California Weather'],\n    ['tnwx', 'Tennessee Weather'],\n    ['azwx', 'Arizona Weather'],\n    ['alwx', 'Alabama Weather'],\n    ['wordpressdotcom', 'wordpress'],\n    ['usNWSgov', 'United States National Weather Service'],\n    ['Suruc', 'Sanliurfa'],\n\n    # Grouping same words without embeddings\n    ['Bestnaijamade', 'bestnaijamade'],\n    ['SOUDELOR', 'Soudelor'],\n]\n\n## Replace using flashtext instead of regex as it is many folds faster in text search and text replace\n# Link: https://github.com/vi3k6i5/flashtext\n\nkp_replace_clean = KeywordProcessor()\nfor elem in text_clean_replace_list:\n    kp_replace_clean.add_keyword(elem[0].strip(), elem[1].strip())\ntest__ =\"Hi this is don't testing on http://www.kaggle.com/rtatman/import-functions-from-kaggle-script and not this\"\n\ndef clean(string):\n    kp_replace_clean.replace_keywords(string)\n    return re.sub(r\"https?:[^\\s]*\", \"website\", string, re.I)\n    \nclean(test__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Cleaning the text columns\ndf_train[\"text\"]=df_train[\"text\"].progress_apply(lambda x: clean(x))\ndf_train[\"selected_text\"]=df_train[\"selected_text\"].progress_apply(lambda x: clean(x))\ndf_test[\"text\"]=df_test[\"text\"].progress_apply(lambda x: clean(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"text_tok\"]=df_train[\"text\"].progress_apply(lambda x: [elem.orth_ for elem in nlp(x)])\ndf_train[\"len_text_tok\"]=df_train[\"text_tok\"].progress_apply(lambda x: len(x))\n\ndf_train[\"selected_text_tok\"]=df_train[\"selected_text\"].progress_apply(lambda x: [elem.orth_ for elem in nlp(x)])\ndf_train[\"len_selected_text_tok\"]=df_train[\"selected_text_tok\"].progress_apply(lambda x: len(x))\n\ndf_test[\"text_tok\"]=df_test[\"text\"].progress_apply(lambda x: [elem.orth_ for elem in nlp(x)])\ndf_test[\"len_text_tok\"]=df_test[\"text_tok\"].progress_apply(lambda x: len(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train[df_train[\"len_text_tok\"]>0]\ndf_train.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.to_pickle(\"df_train.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby(\"sentiment\").mean()[\"jaccard_scr\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **EDA**"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Remove stopwords just for EDA \nen_stop_new=list(string.punctuation)\nen_stop = list(en_stop) + en_stop_new\n\ndef freq_analysis_df(df, col, top_freq=False):\n    temp_text = ' '.join(df[col])\n    wordcloud2 = WordCloud().generate(temp_text)\n    plt.imshow(wordcloud2)\n    plt.axis(\"off\")\n    plt.show()\n    sns.distplot(df[\"len_\"+col+\"_tok\"])\n        \n    temp_flat = [item.lower().strip() for sublist in df[col+\"_tok\"] for item in sublist if item.lower() not in en_stop]\n    if top_freq!=False:\n        temp = pd.DataFrame(collections.Counter(temp_flat).most_common(top_freq), columns = ['Common_words','count'])\n        display(temp.style.background_gradient(cmap='Blues'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_analysis_df(df_train, \"text\", 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_analysis_df(df_train, \"selected_text\", 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_analysis_df(df_test, \"text\", 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_analysis_df(df_train[df_train[\"sentiment\"]==\"neutral\"], \"text\", 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_analysis_df(df_train[df_train[\"sentiment\"]==\"negative\"], \"text\", 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_analysis_df(df_train[df_train[\"sentiment\"]==\"positive\"], \"text\", 50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Getting unique/non-overlapping words in each of the category\n\npositive_text_list = df_train[df_train[\"sentiment\"]==\"positive\"][\"text_tok\"]\npositive_text_list = [item for sublist in positive_text_list for item in sublist]\nnegative_text_list = df_train[df_train[\"sentiment\"]==\"negative\"][\"text_tok\"]\nnegative_text_list = [item for sublist in negative_text_list for item in sublist]\nneutral_text_list = df_train[df_train[\"sentiment\"]==\"neutral\"][\"text_tok\"]\nneutral_text_list = [item for sublist in neutral_text_list for item in sublist]\n\n\npositive_sel_text_list = df_train[df_train[\"sentiment\"]==\"positive\"][\"selected_text_tok\"]\npositive_sel_text_list = [item for sublist in positive_sel_text_list for item in sublist]\nnegative_sel_text_list = df_train[df_train[\"sentiment\"]==\"negative\"][\"selected_text_tok\"]\nnegative_sel_text_list = [item for sublist in negative_sel_text_list for item in sublist]\nneutral_sel_text_list = df_train[df_train[\"sentiment\"]==\"neutral\"][\"selected_text_tok\"]\nneutral_sel_text_list = [item for sublist in neutral_sel_text_list for item in sublist]\n\n\ntemp_pos_neg_text = list(set(positive_text_list + negative_text_list))\ntemp_neut_neg_text = list(set(neutral_text_list + negative_text_list))\ntemp_neut_pos_text = list(set(neutral_text_list + positive_text_list))\n\ntemp_pos_neg_sel_text = list(set(positive_sel_text_list + negative_sel_text_list))\ntemp_neut_neg_sel_text = list(set(neutral_sel_text_list + negative_sel_text_list))\ntemp_neut_pos_sel_text = list(set(neutral_sel_text_list + positive_sel_text_list))\n\n### Unique words in each of the categories in the \"text\" column\nunique_positive_text_list = [elem for elem in positive_text_list if elem not in (temp_neut_neg_text)]\nunique_negative_text_list = [elem for elem in negative_text_list if elem not in (temp_neut_pos_text)]\nunique_neutral_text_list = [elem for elem in neutral_text_list if elem not in (temp_pos_neg_text)]\n\n### Unique words in each of the categories in the \"selected_text\" column\nunique_positive_text_sel_text_list = [elem for elem in positive_sel_text_list if elem not in (temp_neut_neg_sel_text)]\nunique_negative_text_sel_text_list = [elem for elem in negative_sel_text_list if elem not in (temp_neut_pos_sel_text)]\nunique_neutral_text_sel_text_list = [elem for elem in neutral_sel_text_list if elem not in (temp_pos_neg_sel_text)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Remove stopwords just for EDA \n# en_stop_new=list(string.punctuation)\n# en_stop = list(en_stop) + en_stop_new\n\ndef freq_analysis_list(list_, top_freq=False):\n    temp_text = ' '.join(list_)\n    wordcloud2 = WordCloud().generate(temp_text)\n    plt.imshow(wordcloud2)\n    plt.axis(\"off\")\n    plt.show()\n\n    if top_freq!=False:\n        temp = pd.DataFrame(collections.Counter(list_).most_common(top_freq), columns = ['Common_words','count'])\n        display(temp.style.background_gradient(cmap='Blues'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_analysis_list(unique_neutral_text_sel_text_list, 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_analysis_list(unique_positive_text_sel_text_list, 30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_analysis_list(unique_negative_text_sel_text_list, 30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Solving it as a Question and Answer problem:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Adpated from https://www.kaggle.com/jonathanbesomi/question-answering-starter-pack","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare data in QA format\n# Example-format:\n\none_train_data = {\n    'context': \"This tweet sentiment extraction challenge is great\",\n    'qas': [\n        {\n            'id': \"00001\",\n            'question': \"positive\",\n            'answers': [\n                {\n                    'text': \"is great\",\n                    'answer_start': 43\n                }\n            ]\n        }\n    ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_test_data = {\n    'context': 'last session of the day  website',\n    'qas': [\n        {'question': 'neutral',\n         'id': 'f87dea47db',\n         'is_impossible': False,\n         'answers': [\n             {\n                 'answer_start': 1000000,\n                 'text': '__None__'\n             }\n         ]\n        }\n    ]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_in_qa_format(df_row, data_type):\n    if data_type==\"train\":\n        answer_start = df_row[\"text\"].find(df_row[\"selected_text\"])\n        selected_text = df_row[\"selected_text\"]\n    else:\n        answer_start=1000000\n        selected_text = \"__None__\"\n\n    if type(df_row[\"text\"])!=str:\n        df_row[\"text\"]=\"\"\n    if type(df_row[\"sentiment\"])!=str:\n        df_row[\"sentiment\"]=\"\"\n\n    return {\n        'context': df_row[\"text\"],\n        'qas': [{'id': df_row[\"textID\"],\n                 'question': df_row[\"sentiment\"],\n                 'answers': [{'text': selected_text,\n                              'answer_start': answer_start\n                             }\n                            ]\n                }\n               ]\n    }\nnu=2\ndisplay(data_in_qa_format(df_train.iloc[nu], \"train\"))\ndisplay(data_in_qa_format(df_train.iloc[nu], \"test\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"train_dic\"] = df_train.progress_apply(lambda x: data_in_qa_format(x, \"train\"), axis=1)\ndf_test[\"test_dic\"] = df_test.progress_apply(lambda x: data_in_qa_format(x, \"test\"), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"train_dic\"].tolist()[2:4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[\"test_dic\"].tolist()[2:4]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('train.json', 'w') as outfile:\n    json.dump(df_train[\"train_dic\"].tolist(), outfile)\n\nwith open('test.json', 'w') as outfile:\n    json.dump(df_test[\"test_dic\"].tolist(), outfile)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Install simple-transformers, a tool to train and test transformers model easily.\n\n!pip install simpletransformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the distilbert-base-uncased-distilled-squad model\nuse_cuda = True # whether to use GPU or not\n\nfrom simpletransformers.question_answering import QuestionAnsweringModel\n\nMODEL_PATH = '/kaggle/input/transformers-pretrained-distilbert/distilbert-base-uncased-distilled-squad/'\n\n# Create the QuestionAnsweringModel\nmodel = QuestionAnsweringModel('distilbert', \n                               MODEL_PATH, \n                               args={'reprocess_input_data': True,\n                                     'overwrite_output_dir': True,\n                                     'learning_rate': 5e-5,\n                                     'num_train_epochs': 3,\n                                     'max_seq_length': 192,\n                                     'doc_stride': 64,\n                                     'fp16': False,\n                                    },\n                              use_cuda=use_cuda)\n\nmodel.train_model('train.json')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model.predict(df_test[\"test_dic\"].tolist())\npredictions_df = pd.DataFrame.from_dict(predictions[0])\ndf_test['selected_text'] = predictions_df['answer'].progress_apply(lambda x:x[0])\ndf_test[[\"textID\", \"selected_text\"]].to_csv('submission.csv', index=False)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following is to be done:\n- Improve/Finetune this model\n- Try different methods and models\n\nThanks for your time<br />\n## **ッ**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}